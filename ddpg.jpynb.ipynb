{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gym\nfrom gym import spaces\nfrom gym.spaces import Box\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport os\nn_actions = 7\nn_states = 6\nA = [[0.992,0.0018,0,0,0,0],\n              [0.0023,0.9919,0.0043,0,0,0],\n              [0,-0.0042,1.0009,0.0024,0,0],\n              [0,0,0.0013,0.9979,0,0],\n              [0,0,0,0,0.9972,0],\n              [0,0,0,0,0,0.9953]]\n\nB =[[1.0033,0,0,0,0,0,-0.2175],\n              [0,1.0460,0,0,0,0,-0.0788],\n              [0,0,1.0326,0,0,0,-0.0020],\n              [0,0,0,0.4798,0,0,-0.0669],\n              [0,0,0,0,0.8882,0,0.1273],\n              [0,0,0,0,0,1.1699,-0.1792]]\nQ = np.identity(n_states)\nR =  np.identity(n_actions)\nclass SmallSS(gym.Env):\n    def __init__(self):\n        self.action_space = Box(low = 0 , high = 1, shape=(n_actions,))\n        self.observation_space = Box(low = 50 , high = 300, shape=(n_states,))\n        self.state = np.repeat(60,n_states)\n        self.simulation_length = 1000\n        self.A = A\n        self.B = B\n        self.Q = Q\n        self.R = R\n        self.reference = np.array([150,160,165,170,175,180])\n    def step(self, action):\n        #action = action*100\n        #action = action.clip(-100,100)\n        self.state = A@self.state + B@action \n        self.simulation_length -= 1\n        curr_diff = abs(self.state - self.reference)\n        upplim = np.repeat(220,n_states)\n        lowlim = np.repeat(110,n_states)\n        up_constr = np.mean(self.state-upplim)\n        low_constr = np.mean(lowlim-self.state)\n        reward = (curr_diff@Q@curr_diff) + (action@R@action) + 0.1*up_constr + 0.1*low_constr\n        \n        if self.simulation_length <=0:\n            done = True\n        else:\n            done = False\n\n        self.state = tf.clip_by_value(self.state, 50, 300).numpy()\n        info ={}\n\n        return self.state, reward, done, info \n    \n    def render(self):\n        pass\n    def reset(self):\n        self.state = np.repeat(60,n_states)\n        self.simulation_length = 1000\n        return self.state\n    \nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.optimizers import Adam\n\nimport numpy as np\n\nclass ReplayBuffer:\n    def __init__(self, max_size, input_shape, n_actions):\n        self.mem_size = max_size\n        self.mem_cntr = 0\n        self.state_memory = np.zeros((self.mem_size, *input_shape))\n        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n        self.action_memory = np.zeros((self.mem_size, n_actions))\n        self.reward_memory = np.zeros(self.mem_size)\n        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n        self.state_running_avg = [0]\n        self.state_running_std = [1]\n        self.m1 = [0]\n        self.m2 = [0]\n        self.s1 = [1]\n        self.s2 = [1]\n\n    def store_transition(self, state, action, reward, state_, done):\n        index = self.mem_cntr % self.mem_size\n\n        self.state_memory[index] = state\n        self.new_state_memory[index] = state_\n        self.action_memory[index] = action\n        self.reward_memory[index] = reward\n        self.terminal_memory[index] = done\n\n        self.mem_cntr += 1\n\n    def sample_buffer(self, batch_size):\n        max_mem = min(self.mem_cntr, self.mem_size)\n\n        batch = np.random.choice(max_mem, batch_size, replace=False)\n\n        states = self.state_memory[batch]\n        states_ = self.new_state_memory[batch]\n        actions = self.action_memory[batch]\n        rewards = self.reward_memory[batch]\n        dones = self.terminal_memory[batch]\n        \n        state_mean, state_std = np.mean(states), np.std(states)\n        states  = (states - state_mean)/state_std\n            \n        new_state_mean, new_state_std = np.mean(states_), np.std(states_)\n        states_ = (states_ - new_state_mean)/new_state_std\n        \n        #rewards_mean, rewards_std = np.mean(rewards), np.std(rewards)\n        #rewards = (rewards - rewards_mean)/rewards_std\n        \n        self.m1 = [state_mean]\n        self.m2 = [new_state_mean]\n\n        self.s1 = [state_std]\n        self.s2 = [new_state_std]\n\n        self.state_running_avg = np.mean([self.state_running_avg,self.m1,self.m2],axis=0,dtype=object)\n        self.state_running_std = np.mean([self.state_running_std,self.s1,self.s2],axis=0,dtype=object)\n        \n\n        return states, actions, rewards, states_, dones\n    \n    def getstuff(self):\n            return self.state_running_avg , self.state_running_std\n    \nclass Agent:\n    def __init__(self, input_dims, alpha=0.002, beta=0.0005, env=None,\n                 gamma=0.99, n_actions=7, max_size=1000000, tau=0.005,\n                 fc1=1000, fc2=800, batch_size=128, noise=0.3):\n        self.gamma = gamma\n        self.tau = tau\n        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n        self.batch_size = batch_size\n        self.n_actions = n_actions\n        self.noise = noise\n        self.max_action = env.action_space.high[0]\n        self.min_action = env.action_space.low[0]\n\n        self.actor = ActorNetwork(n_actions=n_actions, name='actor')\n        self.critic = CriticNetwork(name='critic')\n        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor')\n        self.target_critic = CriticNetwork(name='target_critic')\n\n        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n        self.critic.compile(optimizer=Adam(learning_rate=beta))\n        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n\n        self.actor_loss = []\n        self.critic_loss = []\n\n        self.update_network_parameters(tau=1)\n\n    def update_network_parameters(self, tau=None):\n        if tau is None:\n            tau = self.tau\n\n        weights = []\n        targets = self.target_actor.weights\n        for i, weight in enumerate(self.actor.weights):\n            weights.append(weight * tau + targets[i]*(1-tau))\n        self.target_actor.set_weights(weights)\n\n        weights = []\n        targets = self.target_critic.weights\n        for i, weight in enumerate(self.critic.weights):\n            weights.append(weight * tau + targets[i]*(1-tau))\n        self.target_critic.set_weights(weights)\n\n    def remember(self, state, action, reward, new_state, done):\n        self.memory.store_transition(state, action, reward, new_state, done)\n\n    def save_models(self):\n        print('... saving models ...')\n        self.actor.save_weights(self.actor.checkpoint_file)\n        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n        self.critic.save_weights(self.critic.checkpoint_file)\n        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n\n    def load_models(self):\n        print('... loading models ...')\n        self.actor.load_weights(self.actor.checkpoint_file)\n        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n        self.critic.load_weights(self.critic.checkpoint_file)\n        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n\n    def choose_action(self, observation, evaluate=False):\n        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n        actions = self.actor(state)\n        if not evaluate:\n            actions += tf.random.normal(shape=[self.n_actions],\n                                        mean=0.0, stddev=self.noise)\n        # note that if the env has an action > 1, we have to multiply by\n        # max action at some point\n        #actions *= 100\n        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n\n        return actions[0]\n    \n    def reduce_exp_noise(self):\n        self.noise = self.noise*0.9\n\n    def learn(self):\n        if self.memory.mem_cntr < self.batch_size:\n            return\n\n        state, action, reward, new_state, done = \\\n            self.memory.sample_buffer(self.batch_size)\n\n        states = tf.convert_to_tensor(state, dtype=tf.float32)\n        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n\n        with tf.GradientTape() as tape:\n            target_actions = self.target_actor(states_)\n            critic_value_ = tf.squeeze(self.target_critic(\n                                states_, target_actions), 1)\n            critic_value = tf.squeeze(self.critic(states, actions), 1)\n            target = rewards + self.gamma*critic_value_*(1-done)\n            critic_loss = keras.losses.MSE(target, critic_value)\n\n        critic_network_gradient = tape.gradient(critic_loss,\n                                                self.critic.trainable_variables)\n        self.critic.optimizer.apply_gradients(zip(\n            critic_network_gradient, self.critic.trainable_variables))\n\n        with tf.GradientTape() as tape:\n            new_policy_actions = self.actor(states)\n            actor_loss = self.critic(states, new_policy_actions)\n            actor_loss = tf.math.reduce_mean(actor_loss)\n\n        actor_network_gradient = tape.gradient(actor_loss,\n                                               self.actor.trainable_variables)\n        self.actor.optimizer.apply_gradients(zip(\n            actor_network_gradient, self.actor.trainable_variables))\n\n        self.update_network_parameters()\n\n        self.actor_loss.append(actor_loss)\n        self.critic_loss.append(critic_loss)\n\n        import os\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense\n\nclass CriticNetwork(keras.Model):\n    def __init__(self, fc1_dims=1000, fc2_dims=800,\n            name='critic', chkpt_dir='/Users/elenizavrakli/Documents/GitHub/DDPG'):\n        super(CriticNetwork, self).__init__()\n        self.fc1_dims = fc1_dims\n        self.fc2_dims = fc2_dims\n\n        self.model_name = name\n        self.checkpoint_dir = chkpt_dir\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, \n                    self.model_name+'_ddpg')\n\n        self.fc1 = Dense(self.fc1_dims, activation='relu')\n        self.fc2 = Dense(self.fc2_dims, activation='relu')\n        self.q = Dense(1, activation=None)\n\n    def call(self, state, action):\n        action_value = self.fc1(tf.concat([state, action], axis=1))\n        action_value = self.fc2(action_value)\n\n        q = self.q(action_value)\n\n        return q\n\nclass ActorNetwork(keras.Model):\n    def __init__(self, fc1_dims=1000, fc2_dims=800, n_actions=7, name='actor',\n            chkpt_dir='/Users/elenizavrakli/Documents/GitHub/DDPG'):\n        super(ActorNetwork, self).__init__()\n        self.fc1_dims = fc1_dims\n        self.fc2_dims = fc2_dims\n        self.n_actions = n_actions\n\n        self.model_name = name\n        self.checkpoint_dir = chkpt_dir\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, \n                    self.model_name+'_ddpg')\n\n        self.fc1 = Dense(self.fc1_dims, activation='relu')\n        self.fc2 = Dense(self.fc2_dims, activation='relu')\n        self.mu = Dense(self.n_actions, activation='sigmoid',kernel_initializer =  tf.random_uniform_initializer(minval=-0.003, maxval=0.003))\n\n    def call(self, state):\n        prob = self.fc1(state)\n        prob = self.fc2(prob)\n\n        mu = self.mu(prob)\n\n        return mu\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_learning_curve(x, scores, figure_file):\n    running_avg = np.zeros(len(scores))\n    for i in range(len(running_avg)):\n        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n    plt.plot(x, running_avg)\n    plt.title('Running average of previous 100 scores')\n    #plt.savefig(figure_file)\n\n    \n\nif __name__ == '__main__':\n    env = SmallSS()\n    agent = Agent(input_dims=env.observation_space.shape, env=env,\n            n_actions=env.action_space.shape[0])\n    n_games = 600\n\n    figure_file = 'plots/pendulum.png'\n\n    best_score = env.reward_range[0]\n    score_history = []\n    load_checkpoint = False\n\n    if load_checkpoint:\n        n_steps = 0\n        while n_steps <= agent.batch_size:\n            observation = env.reset()\n            action = env.action_space.sample()\n            observation_, reward, done, info = env.step(action)\n            agent.remember(observation, action, reward, observation_, done)\n            n_steps += 1\n        agent.learn()\n        agent.load_models()\n        evaluate = True\n    else:\n        evaluate = False\n\n    for i in range(n_games):\n        max_state = 0\n        observation = env.reset()\n        done = False\n        score = 0\n        while not done:\n            if i<=29:\n                action = env.action_space.sample()\n            else:\n                m,s = agent.memory.getstuff()\n                observation_n = (observation-m)/s\n                action = agent.choose_action(observation_n, evaluate).numpy()\n            if np.mean(observation)>max_state:\n                max_state = np.mean(observation)\n            observation_, reward, done, info = env.step(action)\n            score += reward\n            agent.remember(observation, action, reward, observation_, done)\n            if not load_checkpoint:\n                agent.learn()\n            observation = observation_\n        if i%100 == 0:\n            agent.reduce_exp_noise()\n        \n\n        score_history.append(score)\n        avg_score = np.mean(score_history[-100:])\n\n        if avg_score > best_score:\n            best_score = avg_score\n            #if not load_checkpoint:\n             #   agent.save_models()\n\n        print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n        print('Max mean state of episode',max_state)\n\n    if not load_checkpoint:\n        x = [i+1 for i in range(n_games)]\n        plot_learning_curve(x, score_history, figure_file)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-24T13:27:56.809892Z","iopub.execute_input":"2023-11-24T13:27:56.810263Z","iopub.status.idle":"2023-11-24T23:11:18.276965Z","shell.execute_reply.started":"2023-11-24T13:27:56.810230Z","shell.execute_reply":"2023-11-24T23:11:18.276018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_history=[]\nevaluate = True\nobservation = env.reset()\ndone = False\nscore_history = []\ni=1\ngamma = 0.99\nwhile not done:\n    m,s = agent.memory.getstuff()\n    observation_n = (observation-m)/s\n    action = agent.choose_action(observation_n, evaluate).numpy()\n    observation_, reward, done, info = env.step(action)\n    score += pow(gamma,i)*reward\n    score_history.append(score)\n    observation = observation_\n    state_history.append(observation)\n    i += 1\nimport matplotlib.pyplot as plt\n\nplt.plot([i+1 for i in range(1000)], np.array(state_history)[0:1000,:])\nplt.ylabel('Heater temperatures')\nplt.xlabel('time steps')\nplt.savefig(\"abeer_trajectories.pdf\")","metadata":{"execution":{"iopub.status.busy":"2023-11-24T23:11:18.278866Z","iopub.execute_input":"2023-11-24T23:11:18.279146Z","iopub.status.idle":"2023-11-24T23:11:23.183200Z","shell.execute_reply.started":"2023-11-24T23:11:18.279122Z","shell.execute_reply":"2023-11-24T23:11:23.182281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i+1 for i in range(1000)], np.array(score_history)[0:1000])\nplt.ylabel('Sum of discounted costs')\nplt.xlabel('time steps')\nplt.savefig(\"abeer_cost.pdf\")","metadata":{"execution":{"iopub.status.busy":"2023-11-24T23:11:23.187956Z","iopub.execute_input":"2023-11-24T23:11:23.188232Z","iopub.status.idle":"2023-11-24T23:11:23.541366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i+1 for i in range(np.shape(agent.critic_loss)[0])], np.array(agent.critic_loss))\nplt.title(\"Critic loss\")\nplt.ylabel('Loss')\nplt.xlabel('Training Iteration')\nplt.savefig('abeer_critic.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-11-24T23:11:23.544092Z","iopub.execute_input":"2023-11-24T23:11:23.544364Z","iopub.status.idle":"2023-11-24T23:11:50.436502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i+1 for i in range(np.shape(agent.actor_loss)[0])], np.array(agent.actor_loss))\nplt.title(\"Actor loss\")\nplt.ylabel('Loss')\nplt.xlabel('Training Iteration')\nplt.savefig('abeer_actor.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-11-24T23:11:50.437496Z","iopub.execute_input":"2023-11-24T23:11:50.437772Z","iopub.status.idle":"2023-11-24T23:12:18.073803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nx=np.array(state_history)[999,:]\nref=np.array([150,160,165,170,175,180])\nprint(x)\nprint(x-ref)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T23:56:40.206230Z","iopub.execute_input":"2023-11-24T23:56:40.206988Z","iopub.status.idle":"2023-11-24T23:56:40.215894Z","shell.execute_reply.started":"2023-11-24T23:56:40.206957Z","shell.execute_reply":"2023-11-24T23:56:40.214842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aa=np.append(state_history,np.arange(0,1000)[...,None],1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T00:15:41.059071Z","iopub.execute_input":"2023-11-25T00:15:41.059451Z","iopub.status.idle":"2023-11-25T00:15:41.065956Z","shell.execute_reply.started":"2023-11-25T00:15:41.059423Z","shell.execute_reply":"2023-11-25T00:15:41.064815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(threshold=np.inf)\nprint(aa)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T00:20:44.877222Z","iopub.execute_input":"2023-11-25T00:20:44.878104Z","iopub.status.idle":"2023-11-25T00:20:44.951780Z","shell.execute_reply.started":"2023-11-25T00:20:44.878069Z","shell.execute_reply":"2023-11-25T00:20:44.950828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.shape(state_history)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T00:11:06.674141Z","iopub.execute_input":"2023-11-25T00:11:06.674959Z","iopub.status.idle":"2023-11-25T00:11:06.682086Z","shell.execute_reply.started":"2023-11-25T00:11:06.674924Z","shell.execute_reply":"2023-11-25T00:11:06.681136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score","metadata":{"execution":{"iopub.status.busy":"2023-11-24T23:12:18.086146Z","iopub.execute_input":"2023-11-24T23:12:18.086481Z","iopub.status.idle":"2023-11-24T23:12:18.100832Z"},"trusted":true},"execution_count":null,"outputs":[]}]}