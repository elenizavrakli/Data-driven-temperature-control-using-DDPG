{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-24T13:27:56.810263Z","iopub.status.busy":"2023-11-24T13:27:56.809892Z","iopub.status.idle":"2023-11-24T23:11:18.276965Z","shell.execute_reply":"2023-11-24T23:11:18.276018Z","shell.execute_reply.started":"2023-11-24T13:27:56.810230Z"},"trusted":true},"outputs":[],"source":["import gym\n","from gym import spaces\n","from gym.spaces import Box\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","n_actions = 7\n","n_states = 6\n","A = [[0.992,0.0018,0,0,0,0],\n","              [0.0023,0.9919,0.0043,0,0,0],\n","              [0,-0.0042,1.0009,0.0024,0,0],\n","              [0,0,0.0013,0.9979,0,0],\n","              [0,0,0,0,0.9972,0],\n","              [0,0,0,0,0,0.9953]]\n","\n","B =[[1.0033,0,0,0,0,0,-0.2175],\n","              [0,1.0460,0,0,0,0,-0.0788],\n","              [0,0,1.0326,0,0,0,-0.0020],\n","              [0,0,0,0.4798,0,0,-0.0669],\n","              [0,0,0,0,0.8882,0,0.1273],\n","              [0,0,0,0,0,1.1699,-0.1792]]\n","Q = np.identity(n_states)\n","R =  np.identity(n_actions)\n","class SmallSS(gym.Env):\n","    def __init__(self):\n","        self.action_space = Box(low = 0 , high = 1, shape=(n_actions,))\n","        self.observation_space = Box(low = 50 , high = 300, shape=(n_states,))\n","        self.state = np.repeat(60,n_states)\n","        self.simulation_length = 1000\n","        self.A = A\n","        self.B = B\n","        self.Q = Q\n","        self.R = R\n","        self.reference = np.array([150,160,165,170,175,180])\n","    def step(self, action):\n","        self.state = A@self.state + B@action \n","        self.simulation_length -= 1\n","        curr_diff = abs(self.state - self.reference)\n","        upplim = np.repeat(220,n_states)\n","        lowlim = np.repeat(110,n_states)\n","        up_constr = np.mean(self.state-upplim)\n","        low_constr = np.mean(lowlim-self.state)\n","        reward = (curr_diff@Q@curr_diff) + (action@R@action) + 0.1*up_constr + 0.1*low_constr\n","        \n","        if self.simulation_length <=0:\n","            done = True\n","        else:\n","            done = False\n","\n","        self.state = tf.clip_by_value(self.state, 50, 300).numpy()\n","        info ={}\n","\n","        return self.state, reward, done, info \n","    \n","    def render(self):\n","        pass\n","    def reset(self):\n","        self.state = np.repeat(60,n_states)\n","        self.simulation_length = 1000\n","        return self.state\n","    \n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from tensorflow.keras.optimizers import Adam\n","\n","import numpy as np\n","\n","class ReplayBuffer:\n","    def __init__(self, max_size, input_shape, n_actions):\n","        self.mem_size = max_size\n","        self.mem_cntr = 0\n","        self.state_memory = np.zeros((self.mem_size, *input_shape))\n","        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n","        self.action_memory = np.zeros((self.mem_size, n_actions))\n","        self.reward_memory = np.zeros(self.mem_size)\n","        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n","        self.state_running_avg = [0]\n","        self.state_running_std = [1]\n","        self.m1 = [0]\n","        self.m2 = [0]\n","        self.s1 = [1]\n","        self.s2 = [1]\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_cntr % self.mem_size\n","\n","        self.state_memory[index] = state\n","        self.new_state_memory[index] = state_\n","        self.action_memory[index] = action\n","        self.reward_memory[index] = reward\n","        self.terminal_memory[index] = done\n","\n","        self.mem_cntr += 1\n","\n","    def sample_buffer(self, batch_size):\n","        max_mem = min(self.mem_cntr, self.mem_size)\n","\n","        batch = np.random.choice(max_mem, batch_size, replace=False)\n","\n","        states = self.state_memory[batch]\n","        states_ = self.new_state_memory[batch]\n","        actions = self.action_memory[batch]\n","        rewards = self.reward_memory[batch]\n","        dones = self.terminal_memory[batch]\n","        \n","        state_mean, state_std = np.mean(states), np.std(states)\n","        states  = (states - state_mean)/state_std\n","            \n","        new_state_mean, new_state_std = np.mean(states_), np.std(states_)\n","        states_ = (states_ - new_state_mean)/new_state_std\n","        \n","        self.m1 = [state_mean]\n","        self.m2 = [new_state_mean]\n","\n","        self.s1 = [state_std]\n","        self.s2 = [new_state_std]\n","\n","        self.state_running_avg = np.mean([self.state_running_avg,self.m1,self.m2],axis=0,dtype=object)\n","        self.state_running_std = np.mean([self.state_running_std,self.s1,self.s2],axis=0,dtype=object)\n","        \n","\n","        return states, actions, rewards, states_, dones\n","    \n","    def getstuff(self):\n","            return self.state_running_avg , self.state_running_std\n","    \n","class Agent:\n","    def __init__(self, input_dims, alpha=0.002, beta=0.0005, env=None,\n","                 gamma=0.99, n_actions=7, max_size=1000000, tau=0.005,\n","                 fc1=1000, fc2=800, batch_size=128, noise=0.3):\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n","        self.batch_size = batch_size\n","        self.n_actions = n_actions\n","        self.noise = noise\n","        self.max_action = env.action_space.high[0]\n","        self.min_action = env.action_space.low[0]\n","\n","        self.actor = ActorNetwork(n_actions=n_actions, name='actor')\n","        self.critic = CriticNetwork(name='critic')\n","        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor')\n","        self.target_critic = CriticNetwork(name='target_critic')\n","\n","        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n","        self.critic.compile(optimizer=Adam(learning_rate=beta))\n","        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n","        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n","\n","        self.actor_loss = []\n","        self.critic_loss = []\n","\n","        self.update_network_parameters(tau=1)\n","\n","    def update_network_parameters(self, tau=None):\n","        if tau is None:\n","            tau = self.tau\n","\n","        weights = []\n","        targets = self.target_actor.weights\n","        for i, weight in enumerate(self.actor.weights):\n","            weights.append(weight * tau + targets[i]*(1-tau))\n","        self.target_actor.set_weights(weights)\n","\n","        weights = []\n","        targets = self.target_critic.weights\n","        for i, weight in enumerate(self.critic.weights):\n","            weights.append(weight * tau + targets[i]*(1-tau))\n","        self.target_critic.set_weights(weights)\n","\n","    def remember(self, state, action, reward, new_state, done):\n","        self.memory.store_transition(state, action, reward, new_state, done)\n","\n","    def save_models(self):\n","        print('... saving models ...')\n","        self.actor.save_weights(self.actor.checkpoint_file)\n","        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n","        self.critic.save_weights(self.critic.checkpoint_file)\n","        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n","\n","    def load_models(self):\n","        print('... loading models ...')\n","        self.actor.load_weights(self.actor.checkpoint_file)\n","        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n","        self.critic.load_weights(self.critic.checkpoint_file)\n","        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n","\n","    def choose_action(self, observation, evaluate=False):\n","        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n","        actions = self.actor(state)\n","        if not evaluate:\n","            actions += tf.random.normal(shape=[self.n_actions],\n","                                        mean=0.0, stddev=self.noise)\n","        # note that if the env has an action > 1, we have to multiply by\n","        # max action at some point\n","        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n","\n","        return actions[0]\n","    \n","    def reduce_exp_noise(self):\n","        self.noise = self.noise*0.9\n","\n","    def learn(self):\n","        if self.memory.mem_cntr < self.batch_size:\n","            return\n","\n","        state, action, reward, new_state, done = \\\n","            self.memory.sample_buffer(self.batch_size)\n","\n","        states = tf.convert_to_tensor(state, dtype=tf.float32)\n","        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n","        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n","        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n","\n","        with tf.GradientTape() as tape:\n","            target_actions = self.target_actor(states_)\n","            critic_value_ = tf.squeeze(self.target_critic(\n","                                states_, target_actions), 1)\n","            critic_value = tf.squeeze(self.critic(states, actions), 1)\n","            target = rewards + self.gamma*critic_value_*(1-done)\n","            critic_loss = keras.losses.MSE(target, critic_value)\n","\n","        critic_network_gradient = tape.gradient(critic_loss,\n","                                                self.critic.trainable_variables)\n","        self.critic.optimizer.apply_gradients(zip(\n","            critic_network_gradient, self.critic.trainable_variables))\n","\n","        with tf.GradientTape() as tape:\n","            new_policy_actions = self.actor(states)\n","            actor_loss = self.critic(states, new_policy_actions)\n","            actor_loss = tf.math.reduce_mean(actor_loss)\n","\n","        actor_network_gradient = tape.gradient(actor_loss,\n","                                               self.actor.trainable_variables)\n","        self.actor.optimizer.apply_gradients(zip(\n","            actor_network_gradient, self.actor.trainable_variables))\n","\n","        self.update_network_parameters()\n","\n","        self.actor_loss.append(actor_loss)\n","        self.critic_loss.append(critic_loss)\n","\n","        import os\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from tensorflow.keras.layers import Dense\n","\n","class CriticNetwork(keras.Model):\n","    def __init__(self, fc1_dims=1000, fc2_dims=800,\n","            name='critic', chkpt_dir='/Users/elenizavrakli/Documents/GitHub/DDPG'):\n","        super(CriticNetwork, self).__init__()\n","        self.fc1_dims = fc1_dims\n","        self.fc2_dims = fc2_dims\n","\n","        self.model_name = name\n","        self.checkpoint_dir = chkpt_dir\n","        self.checkpoint_file = os.path.join(self.checkpoint_dir, \n","                    self.model_name+'_ddpg')\n","\n","        self.fc1 = Dense(self.fc1_dims, activation='relu')\n","        self.fc2 = Dense(self.fc2_dims, activation='relu')\n","        self.q = Dense(1, activation=None)\n","\n","    def call(self, state, action):\n","        action_value = self.fc1(tf.concat([state, action], axis=1))\n","        action_value = self.fc2(action_value)\n","\n","        q = self.q(action_value)\n","\n","        return q\n","\n","class ActorNetwork(keras.Model):\n","    def __init__(self, fc1_dims=1000, fc2_dims=800, n_actions=7, name='actor',\n","            chkpt_dir='/Users/elenizavrakli/Documents/GitHub/DDPG'):\n","        super(ActorNetwork, self).__init__()\n","        self.fc1_dims = fc1_dims\n","        self.fc2_dims = fc2_dims\n","        self.n_actions = n_actions\n","\n","        self.model_name = name\n","        self.checkpoint_dir = chkpt_dir\n","        self.checkpoint_file = os.path.join(self.checkpoint_dir, \n","                    self.model_name+'_ddpg')\n","\n","        self.fc1 = Dense(self.fc1_dims, activation='relu')\n","        self.fc2 = Dense(self.fc2_dims, activation='relu')\n","        self.mu = Dense(self.n_actions, activation='sigmoid',kernel_initializer =  tf.random_uniform_initializer(minval=-0.003, maxval=0.003))\n","\n","    def call(self, state):\n","        prob = self.fc1(state)\n","        prob = self.fc2(prob)\n","\n","        mu = self.mu(prob)\n","\n","        return mu\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_learning_curve(x, scores, figure_file):\n","    running_avg = np.zeros(len(scores))\n","    for i in range(len(running_avg)):\n","        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n","    plt.plot(x, running_avg)\n","    plt.title('Running average of previous 100 scores')\n","    #plt.savefig(figure_file)\n","\n","\n","if __name__ == '__main__':\n","    env = SmallSS()\n","    agent = Agent(input_dims=env.observation_space.shape, env=env,\n","            n_actions=env.action_space.shape[0])\n","    n_games = 600\n","\n","    figure_file = 'plots/ddpg_temp.png'\n","\n","    best_score = env.reward_range[0]\n","    score_history = []\n","    load_checkpoint = False\n","\n","    if load_checkpoint:\n","        n_steps = 0\n","        while n_steps <= agent.batch_size:\n","            observation = env.reset()\n","            action = env.action_space.sample()\n","            observation_, reward, done, info = env.step(action)\n","            agent.remember(observation, action, reward, observation_, done)\n","            n_steps += 1\n","        agent.learn()\n","        agent.load_models()\n","        evaluate = True\n","    else:\n","        evaluate = False\n","\n","    for i in range(n_games):\n","        max_state = 0\n","        observation = env.reset()\n","        done = False\n","        score = 0\n","        while not done:\n","            if i<=29:\n","                action = env.action_space.sample()\n","            else:\n","                m,s = agent.memory.getstuff()\n","                observation_n = (observation-m)/s\n","                action = agent.choose_action(observation_n, evaluate).numpy()\n","            if np.mean(observation)>max_state:\n","                max_state = np.mean(observation)\n","            observation_, reward, done, info = env.step(action)\n","            score += reward\n","            agent.remember(observation, action, reward, observation_, done)\n","            if not load_checkpoint:\n","                agent.learn()\n","            observation = observation_\n","        if i%100 == 0:\n","            agent.reduce_exp_noise()\n","        \n","\n","        score_history.append(score)\n","        avg_score = np.mean(score_history[-100:])\n","\n","        if avg_score > best_score:\n","            best_score = avg_score\n","            #if not load_checkpoint:\n","             #   agent.save_models()\n","\n","        print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n","        print('Max mean state of episode',max_state)\n","\n","    if not load_checkpoint:\n","        x = [i+1 for i in range(n_games)]\n","        plot_learning_curve(x, score_history, figure_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T23:11:18.279146Z","iopub.status.busy":"2023-11-24T23:11:18.278866Z","iopub.status.idle":"2023-11-24T23:11:23.183200Z","shell.execute_reply":"2023-11-24T23:11:23.182281Z","shell.execute_reply.started":"2023-11-24T23:11:18.279122Z"},"trusted":true},"outputs":[],"source":["state_history=[]\n","evaluate = True\n","observation = env.reset()\n","done = False\n","score_history = []\n","i=1\n","gamma = 0.99\n","while not done:\n","    m,s = agent.memory.getstuff()\n","    observation_n = (observation-m)/s\n","    action = agent.choose_action(observation_n, evaluate).numpy()\n","    observation_, reward, done, info = env.step(action)\n","    score += pow(gamma,i)*reward\n","    score_history.append(score)\n","    observation = observation_\n","    state_history.append(observation)\n","    i += 1\n","import matplotlib.pyplot as plt\n","\n","plt.plot([i+1 for i in range(1000)], np.array(state_history)[0:1000,:])\n","plt.ylabel('Heater temperatures')\n","plt.xlabel('time steps')\n","#plt.savefig(\"trajectories.pdf\")\n","\n","x=np.array(state_history)[999,:]\n","ref=np.array([150,160,165,170,175,180])\n","print(x)\n","print(x-ref)\n","print(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T23:11:23.188232Z","iopub.status.busy":"2023-11-24T23:11:23.187956Z","iopub.status.idle":"2023-11-24T23:11:23.541366Z"},"trusted":true},"outputs":[],"source":["plt.plot([i+1 for i in range(1000)], np.array(score_history)[0:1000])\n","plt.ylabel('Sum of discounted costs')\n","plt.xlabel('time steps')\n","#plt.savefig(\"cost.pdf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T23:11:23.544364Z","iopub.status.busy":"2023-11-24T23:11:23.544092Z","iopub.status.idle":"2023-11-24T23:11:50.436502Z"},"trusted":true},"outputs":[],"source":["plt.plot([i+1 for i in range(np.shape(agent.critic_loss)[0])], np.array(agent.critic_loss))\n","plt.title(\"Critic loss\")\n","plt.ylabel('Loss')\n","plt.xlabel('Training Iteration')\n","#plt.savefig('critic.pdf')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T23:11:50.437772Z","iopub.status.busy":"2023-11-24T23:11:50.437496Z","iopub.status.idle":"2023-11-24T23:12:18.073803Z"},"trusted":true},"outputs":[],"source":["plt.plot([i+1 for i in range(np.shape(agent.actor_loss)[0])], np.array(agent.actor_loss))\n","plt.title(\"Actor loss\")\n","plt.ylabel('Loss')\n","plt.xlabel('Training Iteration')\n","#plt.savefig('actor.pdf')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
